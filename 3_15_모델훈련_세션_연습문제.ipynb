{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/HwangSiyeon/ESAA_homework/blob/main/3_15_%EB%AA%A8%EB%8D%B8%ED%9B%88%EB%A0%A8_%EC%84%B8%EC%85%98_%EC%97%B0%EC%8A%B5%EB%AC%B8%EC%A0%9C.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **| 모델 훈련 연습 문제**\n",
        "___\n",
        "- 출처 : 핸즈온 머신러닝 Ch04 연습문제 1, 5, 9, 10\n",
        "- 개념 문제의 경우 텍스트 셀을 추가하여 정답을 적어주세요."
      ],
      "metadata": {
        "id": "zCu72vDHGMHo"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **1. 수백만 개의 특성을 가진 훈련 세트에서는 어떤 선형 회귀 알고리즘을 사용할 수 있을까요?**\n",
        "___\n"
      ],
      "metadata": {
        "id": "j3g-_Dq9GiuT"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "확률적 경사 하강법이나 미니배치 경사 하강법 훈련 세트가 메모리 크기에 맞으면 배치 경사 하강법도 가능하다. 하지만 정규 방정식이나 SVD 방법은 계산 복잡도가 특성 개수에 따라 매우 빠르게 증가하기 때문에 사용할 수 없다."
      ],
      "metadata": {
        "id": "n3jB8uEQerFW"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **2. 배치 경사 하강법을 사용하고 에포크마다 검증 오차를 그래프로 나타내봤습니다. 검증 오차가 일정하게 상승되고 있다면 어떤 일이 일어나고 있는 걸까요? 이 문제를 어떻게 해결할 수 있나요?**\n",
        "___"
      ],
      "metadata": {
        "id": "-pDjW5XcHPOt"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "학습률이 너무 크면 모델이 발산할 수 있다. 학습률을 줄이면 된다."
      ],
      "metadata": {
        "id": "48Qpq9ble44g"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **3. 릿지 회귀를 사용했을 때 훈련 오차가 검증 오차가 거의 비슷하고 둘 다 높았습니다. 이 모델에는 높은 편향이 문제인가요, 아니면 높은 분산이 문제인가요? 규제 하이퍼파라미터 $\\alpha$를 증가시켜야 할까요 아니면 줄여야 할까요?**\n",
        "___"
      ],
      "metadata": {
        "id": "nM7JbsLoy7b7"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "모델의 과소적합이 발생한다. 높은 편향이 문제이다. 알파값을 감소시켜야 한다."
      ],
      "metadata": {
        "id": "7QK2oiGSfZby"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **4. 다음과 같이 사용해야 하는 이유는?**\n",
        "___\n",
        "- 평범한 선형 회귀(즉, 아무런 규제가 없는 모델) 대신 릿지 회귀\n",
        "- 릿지 회귀 대신 라쏘 회귀\n",
        "- 라쏘 회귀 대신 엘라스틱넷"
      ],
      "metadata": {
        "id": "C8tARu-ZzOGx"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "- 첫번째 경우는 규제를 강화하여 오버피팅을 방지하기 위해서이다\n",
        "- 두번째 경우는 중요한 특징은 덜 중요한 특성의 가중치를 제거하려고 할 때 사용한다\n",
        "- 세번째 경우에서 엘라스틱넷은 릿지회귀와 랏쏘회귀를 절충한 모델으로,"
      ],
      "metadata": {
        "id": "FIxwZPWgf3IO"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **추가) 조기 종료를 사용한 배치 경사 하강법으로 iris 데이터를 활용해 소프트맥스 회귀를 구현해보세요(사이킷런은 사용하지 마세요)**\n",
        "\n",
        "---\n",
        "\n"
      ],
      "metadata": {
        "id": "QIZpOEYJVIAV"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "from sklearn.datasets import load_iris\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import OneHotEncoder\n",
        "\n",
        "# Load iris dataset\n",
        "iris = load_iris()\n",
        "X, y = iris.data, iris.target\n",
        "\n",
        "# Normalize features\n",
        "X = (X - X.mean(axis=0)) / X.std(axis=0)\n",
        "\n",
        "# One-hot encode the target variable\n",
        "encoder = OneHotEncoder(categories='auto')\n",
        "y_onehot = encoder.fit_transform(y.reshape(-1, 1)).toarray()\n",
        "\n",
        "# Add bias term to features\n",
        "X = np.c_[np.ones((X.shape[0], 1)), X]\n",
        "\n",
        "# Split data into train and test sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y_onehot, test_size=0.2, random_state=42)\n",
        "\n",
        "# Softmax function\n",
        "def softmax(z):\n",
        "    exp_z = np.exp(z - np.max(z, axis=1, keepdims=True))\n",
        "    return exp_z / np.sum(exp_z, axis=1, keepdims=True)\n",
        "\n",
        "# Cross-entropy loss function\n",
        "def cross_entropy_loss(y_true, y_pred):\n",
        "    return -np.mean(np.sum(y_true * np.log(y_pred), axis=1))\n",
        "\n",
        "# Gradient of cross-entropy loss function\n",
        "def grad_cross_entropy_loss(y_true, y_pred):\n",
        "    return y_pred - y_true\n",
        "\n",
        "# Batch gradient descent with early stopping\n",
        "def batch_gradient_descent(X, y, learning_rate, epochs, tolerance):\n",
        "    np.random.seed(42)\n",
        "    theta = np.random.randn(X.shape[1], y.shape[1])\n",
        "    best_loss = float('inf')\n",
        "    best_epoch = 0\n",
        "    for epoch in range(epochs):\n",
        "        logits = np.dot(X, theta)\n",
        "        y_pred = softmax(logits)\n",
        "        loss = cross_entropy_loss(y, y_pred)\n",
        "        if epoch % 100 == 0:\n",
        "            print(f\"Epoch {epoch}: Loss {loss}\")\n",
        "        if loss < best_loss - tolerance:\n",
        "            best_loss = loss\n",
        "            best_epoch = epoch\n",
        "        elif epoch - best_epoch > 1000:\n",
        "            print(\"Early stopping.\")\n",
        "            break\n",
        "        gradient = np.dot(X.T, grad_cross_entropy_loss(y, y_pred)) / len(X)\n",
        "        theta -= learning_rate * gradient\n",
        "    return theta\n",
        "\n",
        "# Train softmax regression model\n",
        "learning_rate = 0.01\n",
        "epochs = 10000\n",
        "tolerance = 1e-6\n",
        "theta = batch_gradient_descent(X_train, y_train, learning_rate, epochs, tolerance)\n",
        "\n",
        "# Make predictions on test set\n",
        "logits_test = np.dot(X_test, theta)\n",
        "y_pred_test = softmax(logits_test)\n",
        "predicted_labels = np.argmax(y_pred_test, axis=1)\n",
        "\n",
        "# Calculate accuracy\n",
        "accuracy = np.mean(predicted_labels == np.argmax(y_test, axis=1))\n",
        "print(f\"Accuracy on test set: {accuracy}\")\n"
      ],
      "metadata": {
        "id": "8pXDQ_fU8Nz0"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}